{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets accelerate peft trl einops\n!pip install -U bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-04T15:47:22.342227Z","iopub.execute_input":"2024-11-04T15:47:22.342576Z","iopub.status.idle":"2024-11-04T15:48:04.982747Z","shell.execute_reply.started":"2024-11-04T15:47:22.342544Z","shell.execute_reply":"2024-11-04T15:48:04.981520Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nCollecting trl\n  Downloading trl-0.12.0-py3-none-any.whl.metadata (10 kB)\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\nCollecting transformers\n  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading trl-0.12.0-py3-none-any.whl (310 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops, transformers, trl, peft\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed einops-0.8.0 peft-0.13.2 transformers-4.46.1 trl-0.12.0\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nimport time\nfrom datasets import load_dataset\nfrom peft import LoraConfig, prepare_model_for_kbit_training, PeftModel, get_peft_model\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer\n)\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom accelerate import Accelerator\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-11-04T16:20:30.217861Z","iopub.execute_input":"2024-11-04T16:20:30.218811Z","iopub.status.idle":"2024-11-04T16:20:30.225502Z","shell.execute_reply.started":"2024-11-04T16:20:30.218738Z","shell.execute_reply":"2024-11-04T16:20:30.224546Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# import wandb\n# from kaggle_secrets import UserSecretsClient\n# user_secrets = UserSecretsClient()\n# secret_value_0 = user_secrets.get_secret(\"wandb_api_key\")\n# wandb.login(key=secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T15:48:27.395554Z","iopub.execute_input":"2024-11-04T15:48:27.395853Z","iopub.status.idle":"2024-11-04T15:48:28.504497Z","shell.execute_reply.started":"2024-11-04T15:48:27.395822Z","shell.execute_reply":"2024-11-04T15:48:28.503659Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import os\n# disable Weights and Biases\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"execution":{"iopub.status.busy":"2024-11-04T16:20:33.686046Z","iopub.execute_input":"2024-11-04T16:20:33.686716Z","iopub.status.idle":"2024-11-04T16:20:33.692205Z","shell.execute_reply.started":"2024-11-04T16:20:33.686678Z","shell.execute_reply":"2024-11-04T16:20:33.691259Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Load SNLI dataset\ndataset = load_dataset(\"snli\")\n\n# Select indices for sampling\ntrain_indices = list(range(0, len(dataset[\"train\"]), 550))[:1000]\nvalidation_indices = list(range(0, len(dataset[\"validation\"]), 100))[:100]\ntest_indices = list(range(0, len(dataset[\"test\"]), 100))[:100]\n\n# Subset datasets using the selected indices\ntrain_data = dataset[\"train\"].select(train_indices)\nvalidation_data = dataset[\"validation\"].select(validation_indices)\ntest_data = dataset[\"test\"].select(test_indices)\n\n# Remove entries with label -1 from the validation dataset\nvalid_labels = [0, 1, 2]\n\n# Filter the validation dataset\nvalidation_data = validation_data.filter(lambda x: x['label'] in valid_labels)\n\n# Model and tokenizer\nbase_model = \"microsoft/phi-2\"\ntokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Quantization configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    low_cpu_mem_usage=True,\n    device_map={\"\": 0}\n)\n\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=8,\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T16:20:37.766219Z","iopub.execute_input":"2024-11-04T16:20:37.767063Z","iopub.status.idle":"2024-11-04T16:20:44.157243Z","shell.execute_reply.started":"2024-11-04T16:20:37.767020Z","shell.execute_reply":"2024-11-04T16:20:44.156264Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1893bb932094837baebaad6354e0b7a"}},"metadata":{}}]},{"cell_type":"code","source":"# Prompt formatting function\ndef format_prompt(premise, hypothesis, label=None):\n    label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n    if label is not None:\n        # During training, include the label\n        return f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nLabel: {label_map[label]}\"\n    else:\n        # During inference, we leave out the label\n        return f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nLabel:\"\n\n# Preprocess the dataset with tokenization and create input-target pairs\ndef preprocess_function(examples, tokenizer, is_train=True):\n    inputs = [format_prompt(p, h, label if is_train else None) for p, h, label in zip(examples['premise'], examples['hypothesis'], examples['label'])]\n    model_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=150)\n\n    if is_train:\n        labels = model_inputs['input_ids'].copy() \n        model_inputs['labels'] = labels\n    else:\n        model_inputs['labels'] = model_inputs['input_ids'].copy()\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-11-04T16:23:28.735029Z","iopub.execute_input":"2024-11-04T16:23:28.735411Z","iopub.status.idle":"2024-11-04T16:23:28.744646Z","shell.execute_reply.started":"2024-11-04T16:23:28.735377Z","shell.execute_reply":"2024-11-04T16:23:28.743728Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# import os\n# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encodings = preprocess_function(train_data, tokenizer, is_train=True)\nvalidation_encodings = preprocess_function(validation_data, tokenizer, is_train=False)\ntest_encodings = preprocess_function(test_data, tokenizer, is_train=False)\n\n# Custom Dataset Class\nclass NLIDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        # Convert each encoding dictionary item to tensor format\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\n# Create train, validation, and test datasets\ntrain_dataset = NLIDataset(train_encodings)\nvalidation_dataset = NLIDataset(validation_encodings)\ntest_dataset = NLIDataset(test_encodings)\n\n# Training arguments\ntrain_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=5,\n    load_best_model_at_end=True,\n)\n\n# Custom metric calculation for accuracy\ndef compute_metrics(predictions, labels):\n    pred_labels = np.argmax(predictions, axis=1)\n    accuracy = accuracy_score(labels, pred_labels)\n    return {\"accuracy\": accuracy}\n\n# Trainer for fine-tuning\ntrainer = Trainer(\n    model=model,\n    args=train_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    compute_metrics=lambda p: compute_metrics(p.predictions, p.label_ids)\n)\n\n# Fine-tune the model\nstart_time = time.time()\ntrainer.train()\nend_time = time.time()\n\n# Save the final model\nmodel.save_pretrained('./fine_tuned_model')\nprint(\"Time taken to fine-tune the model:\", end_time - start_time)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the fine-tuned model for evaluation\nfine_tuned_model = PeftModel.from_pretrained(model, './fine_tuned_model', is_trainable=False)\n\n# Total parameters in the model\ntotal_params = sum(p.numel() for p in fine_tuned_model.parameters())\n# Count trainable parameters\ntrainable_params = sum(p.numel() for p in fine_tuned_model.parameters() if p.requires_grad)\n\nprint(f\"Total Parameters: {total_params}, Trainable Parameters: {trainable_params}\")\n\n\n#evaluation-only TrainingArguments for pretrained and fine-tuned model evaluation\neval_args = TrainingArguments(\n    output_dir='./eval_results',\n    per_device_eval_batch_size=4,\n    do_train=False,               \n    do_eval=True,                 \n    logging_dir='./eval_logs',    \n    report_to=None                \n)\n\n# Evaluation trainer for the fine-tuned model\neval_trainer = Trainer(\n    model=fine_tuned_model,\n    args=eval_args,\n    eval_dataset=test_dataset,\n    compute_metrics=lambda p: compute_metrics(p.predictions, p.label_ids)\n)\neval_results = eval_trainer.evaluate()\nprint(\"Fine-tuned Model Accuracy on Test Set:\", eval_results[\"eval_accuracy\"])\n\n# Evaluation trainer for the pretrained model\npretrained_model = AutoModelForCausalLM.from_pretrained(base_model)\npretrained_trainer = Trainer(\n    model=pretrained_model,\n    args=eval_args,\n    eval_dataset=test_dataset,\n    compute_metrics=lambda p: compute_metrics(p.predictions, p.label_ids)\n)\n\npretrained_eval_results = pretrained_trainer.evaluate()\nprint(\"Pretrained Model Accuracy on Test Set:\", pretrained_eval_results[\"eval_accuracy\"])\n\nprint(f\"Accuracy Comparison:\\n- Pretrained Model: {pretrained_eval_results['eval_accuracy']}\\n- Fine-tuned Model: {eval_results['eval_accuracy']}\")\n","metadata":{},"execution_count":null,"outputs":[]}]}